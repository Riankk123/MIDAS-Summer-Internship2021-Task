{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment_Log_For_MIDAS_Task.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntjIFNUXaiqu"
      },
      "source": [
        "# Experiment Log\n",
        "## Step 1\n",
        "<ul>\n",
        "<li>I uploaded the dataset , resized it to an appropriate size and wrote the code for the model .</li>\n",
        "<li>Initially the model contained a lot of parameters (approximately 30,000,000), i.e., it was a very deep model . </li>\n",
        "<li>Then I trained the model on the images .</li>\n",
        "<li>The results were not very impressive , the training accuracy was around 92% whereas the vaidation accuracy was only 2%. </li>\n",
        "<li>I think the reasons for it were :\n",
        "    <ol>\n",
        "    <li>The model had a lot of parameters , compared to the complexity of the images .</li>\n",
        "    <li>The dataset was very small . It only contained about 1900 training images for 62 classes .</li>\n",
        "    </ol>\n",
        "</li> \n",
        "<li>Now , In order to increase the validation accuracy , I reduced the the Number of parameters to around 80,000 . The results were:\n",
        "  <ol>\n",
        "  <li>The validation accuracy increased from 2% to (10 to 13)% , whereas the training accuracy decreased to some extent . I was able to slightly reduce overfitting in the model .</li>\n",
        "  </ol>\n",
        "</li>\n",
        "<li>\n",
        "In order to increase the validation accuracy further, I removed the unnecessary background which contained the majority of the image part .Basically I cropped the image , so that it contained only the numbers and alphabets . \n",
        "</li>\n",
        "<li>The validation accuracy improved to 30-35 % .</li>\n",
        "<li>Now to increase it further , I realized that I need to increase the number of different images being fed into the model .</li>\n",
        "<li>So , I used the ImageDataGeneratorFunction (Used for Data Augmentation) to feed different images which were changed through rotations and shifts to increase the variety of images the model is being trained on . </li>\n",
        "<li> Now in the 11th epoch I was able to find my best fit model which had around ~78 % accuracy on train set and ~78 % accuracy on validation set also .</li>\n",
        "<li> I saved the checkpoints of these model , plotted the graphs for validation and train loss and accuracy , and proceeded to task 2 .</li>\n",
        "<li> I made another attempt to increase the validation set , by making a deeper model with skip connections . But the model was overfitting therefore , I finalised the previous model . </li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "## Step 2\n",
        "<ul>\n",
        "<li> I uploaded the dataset and used the 11th epoch of the saved model of step 1 as the pre-trained model and wrote the code for the randomly initialised model .</li>\n",
        "<li> Saved the checkpoints of both the model and analysed the training quality using Accuracy, Precision and Recall metrics .</li>\n",
        "<li> Took the inference from the plotted graphs of the pre-trained and randomly initialised model to compare the convergence times .</li>\n",
        "</ul>\n",
        "\n",
        "## Step 3\n",
        "<ul>\n",
        "<li> I uploaded the dataset and used the 11th epoch of the saved model of step 1 as the pre-trained model and wrote the code for the randomly initialised model .</li>\n",
        "<li> Saved the checkpoints of both the model and analysed the training quality using Accuracy, Precision and Recall metrics .</li>\n",
        "<li> Took the inference from the plotted graphs of the pre-trained and randomly initialised model to compare the convergence times .</li>\n",
        "<li> This time the dataset was really very different , since it contained a lot of noise where most of the images were not mapped to it's true labels .</li>\n",
        "<li> Therefore analysed the effect of these different dataset on the training quality metrics . </li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGQ9AVM_0VjF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}